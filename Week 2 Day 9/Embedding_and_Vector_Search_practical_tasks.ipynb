{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F96VRX22-IkX"
      },
      "source": [
        "#**Create local vector embeddings using sentens-transformer python library**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGYeMFL2bojw"
      },
      "source": [
        "##**GOAL: to embed text sentences and perform semantic searches using your own Python code.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOPrcTxuFWJl"
      },
      "source": [
        "There are many pre-trained embedding models available on Hugging Face that you can use to create vector embeddings.\n",
        "Sentence Transformers (SBERT) is a library that makes it easy to use these models for vector embedding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsqP-DgCGHcB"
      },
      "source": [
        "Use pip  to install  'sentence_transformers' library  and import  'SentenceTransformer model loader' from this library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "ZEGQgMdBFglD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\edvard.smoliakov\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "c:\\Users\\edvard.smoliakov\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(np, \"object\"):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\edvard.smoliakov\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rY-ID8AfHKYX"
      },
      "source": [
        "Load the 'paraphrase-MiniLM-L6-v2' model  from HuggingFace resource  using the  SentenceTransformer( *model-name* )  and store the reference to the model object in the 'model' variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "m2ob6UwMMThc"
      },
      "outputs": [],
      "source": [
        "# place your code here\n",
        "model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6pgQE3aMkK_"
      },
      "source": [
        "After loading the model, call the 'encode()' method on the model object to create a vector representation of a specific text sentence. Use your own text string  as the parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IxlctyNHZVtJ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.16242452, -0.32998833, -0.26105827, -0.27376795,  0.2055108 ,\n",
              "       -0.3268377 , -0.2852706 ,  0.05449158, -0.45635918, -0.0854828 ,\n",
              "       -0.3885019 , -0.04291125, -0.1490832 ,  0.1757705 , -0.2649453 ,\n",
              "       -0.05682654,  0.17586823, -0.7168272 , -0.3233005 , -0.260863  ,\n",
              "       -0.07775095, -0.4677556 ,  0.19411592, -0.263966  ,  0.2602248 ,\n",
              "        0.24813132,  0.3115436 , -0.06603002,  0.28283894, -0.38867858,\n",
              "        0.3456558 , -0.16287027,  0.29766524, -0.08733153, -0.2163121 ,\n",
              "        0.376099  , -0.02729485, -0.4152918 ,  0.17746793,  0.07043826,\n",
              "       -0.19648837, -0.34868944,  0.326831  , -0.1860769 ,  0.7209968 ,\n",
              "        0.07007735,  0.06338777, -0.60558605,  0.14234878, -0.08384811,\n",
              "       -0.11462647, -0.20250504,  0.2791814 , -0.11856436, -0.24774249,\n",
              "        0.47772932,  0.5065195 , -0.5612685 , -0.43987763, -0.32156006,\n",
              "       -0.07674744, -0.08610552,  0.32706884,  0.4351158 ,  0.52870744,\n",
              "       -0.2865257 ,  0.4747733 ,  0.16286393,  0.02088573,  0.20160536,\n",
              "       -0.31478754, -0.13336597, -0.1616114 ,  0.19989704,  0.6528457 ,\n",
              "       -0.16650932, -0.08848701, -0.14424875,  0.5252512 ,  0.2083844 ,\n",
              "        0.16599701, -0.7260321 , -0.19518311,  0.09762657,  0.09790035,\n",
              "       -0.1106011 ,  0.0372069 , -0.30309293, -0.05033645,  0.19691758,\n",
              "        0.3232997 , -0.34194696, -0.55916804,  0.27300778,  0.89598507,\n",
              "        0.332196  , -0.4101626 , -0.06670186,  0.08952958,  0.12422794,\n",
              "       -0.27067953, -0.33688983, -0.38389003, -0.05730167, -0.3751733 ,\n",
              "       -0.07905708,  0.13383175,  0.14236325,  0.18277226, -0.0328508 ,\n",
              "       -0.00957614, -0.01961563,  0.2829816 , -0.19451563,  0.18090548,\n",
              "        0.663293  , -0.26101375,  0.4641805 ,  0.10719069,  0.37026533,\n",
              "       -0.28428683, -0.4421847 ,  0.31747139,  0.27263942,  0.03088382,\n",
              "       -0.43049785, -0.48964742, -0.14390428, -0.19158755, -0.4734224 ,\n",
              "       -0.05077283, -0.01262977, -0.18531878, -0.18576902,  0.05811208,\n",
              "        0.31575695,  0.14804348, -0.11242983, -0.36851192,  0.4172738 ,\n",
              "       -0.07995725,  0.6564365 ,  0.41560474, -0.60556704, -0.37093902,\n",
              "        0.46110424, -0.18779904,  0.1346109 , -0.18571351, -0.27571276,\n",
              "       -0.23252319,  0.17848808,  0.4223741 ,  0.00752165,  0.44718456,\n",
              "        0.0295712 ,  0.1178351 ,  0.3392732 , -0.24184781,  0.07158384,\n",
              "       -0.51317364,  0.10327366, -0.18447657,  0.3413595 ,  0.01960448,\n",
              "       -0.10428401,  0.11370651,  0.1645282 , -0.26178718,  0.3962328 ,\n",
              "       -0.03979441, -0.13085197,  0.4015812 ,  0.28267956,  0.55453795,\n",
              "       -0.29280478, -0.4077579 , -0.23501752, -0.08090948,  0.1672812 ,\n",
              "        0.46052825, -0.19631149,  0.5964617 ,  0.29207608,  0.21073341,\n",
              "        0.12680559,  0.17278565,  0.3028892 , -0.24911265,  0.2258168 ,\n",
              "       -0.44502115,  0.9353202 , -0.15327054,  0.07148246, -0.10574048,\n",
              "        0.2830803 ,  0.48975435,  0.12511541, -0.8727975 ,  0.3996925 ,\n",
              "       -0.07816254, -0.72077805, -0.09064463, -0.00445372, -0.15918887,\n",
              "       -0.6777756 , -0.24795441, -0.06025371, -0.32387766,  0.36751643,\n",
              "        0.11858021, -0.3223727 , -0.15338106,  0.23186457,  0.09259192,\n",
              "       -0.39048684,  0.00169327, -0.02280916, -0.17175281, -0.20363003,\n",
              "       -0.02943789,  0.22308032,  0.19891977, -0.37850222,  0.00688881,\n",
              "       -0.4516811 , -0.8265562 ,  0.08152002,  0.13218644,  0.3564388 ,\n",
              "        0.3409052 ,  0.47923446,  0.11960161,  0.42020652, -0.0530039 ,\n",
              "       -0.14411072,  0.08547673,  0.15653591,  0.05408547, -0.37879   ,\n",
              "       -0.19918007, -0.41691113, -0.08180193,  0.2581558 ,  0.9417095 ,\n",
              "        0.5986033 ,  0.07951503, -0.55471236, -0.11432268,  0.5231797 ,\n",
              "       -0.22571999, -0.08941948,  0.1333476 ,  0.20923   ,  0.00778288,\n",
              "       -0.12891054, -0.36189184, -0.04302998, -0.00945542,  0.3131388 ,\n",
              "        0.45688626, -0.11451273,  0.25742745, -0.41350314, -0.09204093,\n",
              "       -0.24893059, -0.42372426, -0.05737315,  0.10273814,  0.30546594,\n",
              "       -0.21357404, -0.22777736, -0.17791836, -0.2975582 , -0.10732784,\n",
              "        0.0545649 ,  0.12412992, -0.448737  , -0.17668295, -0.42362595,\n",
              "       -0.19124903, -0.03009627,  0.01334271,  0.19059221,  0.20377338,\n",
              "        0.05646345, -0.11580352, -0.10793757, -0.2981776 , -0.1996886 ,\n",
              "        0.07210393,  0.36311042, -0.7118743 ,  0.36762965, -0.21509735,\n",
              "        0.20231372,  0.3284723 ,  0.22260553, -0.0519363 ,  0.02579337,\n",
              "        0.587799  ,  0.4883834 , -0.03266365,  0.13133596, -0.40549946,\n",
              "       -0.08643522,  0.22465938,  0.20368515, -0.10557435,  0.7018259 ,\n",
              "        0.19299054, -0.10013243, -0.09847789, -0.02218422,  0.00374286,\n",
              "        0.02458319,  0.00417419, -0.35851374, -0.1449822 , -0.26071522,\n",
              "        0.00686259, -0.2284612 ,  0.23744965,  0.4630867 , -0.454136  ,\n",
              "        0.29965162, -0.7343593 ,  0.39781484, -0.12124332, -0.2935705 ,\n",
              "        0.2782706 , -0.16482066,  0.01496719,  1.0408214 ,  0.22085984,\n",
              "        0.37942743, -0.19173788,  0.29561067, -0.16908036, -0.26103356,\n",
              "        0.19079243, -0.01051815,  0.20348144,  0.42937478, -0.32853988,\n",
              "        0.23144412,  0.20519455, -0.29625663, -0.29661113,  0.28503892,\n",
              "       -0.66270286,  0.12592672,  0.15733717,  0.07792787,  0.4448718 ,\n",
              "        0.4485011 ,  0.00930668, -0.23093121, -0.06960053, -0.18429154,\n",
              "        0.02481474, -0.17444502,  0.32450256,  0.22307457,  0.7535445 ,\n",
              "        0.20905523, -0.20774019, -0.22251678,  0.332068  ,  0.03128548,\n",
              "        0.20897521, -0.20536222, -0.23931988, -0.01054061,  0.7130544 ,\n",
              "        0.09713347, -0.24467349, -0.05571305, -0.714817  ,  0.61919826,\n",
              "        0.36945495,  0.05685853,  0.05660712, -0.0115609 ], dtype=float32)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# complete the code\n",
        "sentence = \"The AI engineering course is very intense but interesting.\"  #your sentence\n",
        "embedding = model.encode(sentence)\n",
        "embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql9pFPX4SsnZ"
      },
      "source": [
        "Create vector representations for several text sentences. Place the text strings in a list and use this list as an argument. Use 8-10 sentences of 20-25 words each.  Call the 'encode()' method on the model object with the list of sentences as an argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "t0uEOYZDTvdr"
      },
      "outputs": [],
      "source": [
        "# complete the code\n",
        "sentences_list = [    \n",
        "    \"Machine learning algorithms can identify complex patterns in large datasets that would be impossible for humans to detect manually.\",\n",
        "    \"Natural language processing enables computers to understand, interpret, and generate human language in a meaningful and useful way.\",\n",
        "    \"Deep neural networks are inspired by the structure of the human brain and consist of many interconnected layers of nodes.\",\n",
        "    \"Transfer learning allows a model pre-trained on one task to be fine-tuned efficiently for a completely different target task.\",\n",
        "    \"Vector embeddings represent words or sentences as dense numerical arrays that capture semantic meaning and contextual relationships.\",\n",
        "    \"Reinforcement learning trains agents to make sequential decisions by rewarding desired behaviors and penalizing undesired ones over time.\",\n",
        "    \"Data preprocessing is a critical step in any machine learning pipeline because garbage input always produces garbage output results.\",\n",
        "    \"Transformer architectures revolutionized NLP by using self-attention mechanisms to model long-range dependencies across entire input sequences.\",\n",
        "    \"Semantic search retrieves documents based on the meaning of a query rather than simple keyword overlap or exact string matching.\",\n",
        "    \"Model evaluation metrics such as precision, recall, and F1-score help us understand how well a classifier is really performing.\",]    #your sentences\n",
        "\n",
        "embeddings = model.encode(sentences_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIATXYYKWNSe"
      },
      "source": [
        "#**Definition of semantic textual similaritye**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "addEfKLoXawa"
      },
      "source": [
        "Import 'util' module from sentence_transformers library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IDwL4gFsXogP"
      },
      "outputs": [],
      "source": [
        "# place your code here\n",
        "from sentence_transformers import util"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPbGmfW4YJoZ"
      },
      "source": [
        "You can calculate the cosine similarity of the vector representations of our sentences using the 'cos_sim()' function from the util module.\n",
        "Example: sim = util.cos_sim(embedding_1, embedding_2). Calculate the cosine similarity for any two sentences from your list.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jtN5ElQKZYor"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.2953]])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# place your code here\n",
        "embedding_1 = embeddings[0]\n",
        "embedding_2 = embeddings[1]\n",
        "\n",
        "sim = util.cos_sim(embedding_1, embedding_2)\n",
        "sim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZPd8fUmazhu"
      },
      "source": [
        "Write and test a function named 'cos_similarity_calculation' that determines the semantic similarity between the sentences in your list and any text sentence using their vector representations and the cosine distance as a similarity measure.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "gZuUCPGBuswG"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0.5002042055130005,\n",
              "  'Machine learning algorithms can identify complex patterns in large datasets that would be impossible for humans to detect manually.'),\n",
              " (0.49907559156417847,\n",
              "  'Deep neural networks are inspired by the structure of the human brain and consist of many interconnected layers of nodes.'),\n",
              " (0.4459686279296875,\n",
              "  'Natural language processing enables computers to understand, interpret, and generate human language in a meaningful and useful way.'),\n",
              " (0.4378020167350769,\n",
              "  'Data preprocessing is a critical step in any machine learning pipeline because garbage input always produces garbage output results.'),\n",
              " (0.4335484504699707,\n",
              "  'Transfer learning allows a model pre-trained on one task to be fine-tuned efficiently for a completely different target task.'),\n",
              " (0.39404425024986267,\n",
              "  'Model evaluation metrics such as precision, recall, and F1-score help us understand how well a classifier is really performing.'),\n",
              " (0.3759359121322632,\n",
              "  'Reinforcement learning trains agents to make sequential decisions by rewarding desired behaviors and penalizing undesired ones over time.'),\n",
              " (0.34212660789489746,\n",
              "  'Transformer architectures revolutionized NLP by using self-attention mechanisms to model long-range dependencies across entire input sequences.'),\n",
              " (0.29206690192222595,\n",
              "  'Vector embeddings represent words or sentences as dense numerical arrays that capture semantic meaning and contextual relationships.'),\n",
              " (0.19688329100608826,\n",
              "  'Semantic search retrieves documents based on the meaning of a query rather than simple keyword overlap or exact string matching.')]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# place your code here\n",
        "def cos_similarity_calculation(text, sentences, embeddings, model):\n",
        "    text_embed = model.encode(text)\n",
        "\n",
        "    results = []\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        score = util.cos_sim(text_embed, embeddings[i]).item()\n",
        "        results.append((score, sentence))\n",
        "    \n",
        "    results.sort(reverse=True)\n",
        "\n",
        "    return results\n",
        "\n",
        "text = \"Machine learning uses lots of tools.\"\n",
        "cos_similarity_calculation(text, sentences_list, embeddings, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0.16381791234016418,\n",
              "  'Semantic search retrieves documents based on the meaning of a query rather than simple keyword overlap or exact string matching.'),\n",
              " (0.064333476126194,\n",
              "  'Reinforcement learning trains agents to make sequential decisions by rewarding desired behaviors and penalizing undesired ones over time.'),\n",
              " (0.03935271501541138,\n",
              "  'Data preprocessing is a critical step in any machine learning pipeline because garbage input always produces garbage output results.'),\n",
              " (0.03696758300065994,\n",
              "  'Natural language processing enables computers to understand, interpret, and generate human language in a meaningful and useful way.'),\n",
              " (0.009597387164831161,\n",
              "  'Machine learning algorithms can identify complex patterns in large datasets that would be impossible for humans to detect manually.'),\n",
              " (-0.011903373524546623,\n",
              "  'Vector embeddings represent words or sentences as dense numerical arrays that capture semantic meaning and contextual relationships.'),\n",
              " (-0.03747481480240822,\n",
              "  'Model evaluation metrics such as precision, recall, and F1-score help us understand how well a classifier is really performing.'),\n",
              " (-0.04625102877616882,\n",
              "  'Transfer learning allows a model pre-trained on one task to be fine-tuned efficiently for a completely different target task.'),\n",
              " (-0.04784221202135086,\n",
              "  'Deep neural networks are inspired by the structure of the human brain and consist of many interconnected layers of nodes.'),\n",
              " (-0.12286735326051712,\n",
              "  'Transformer architectures revolutionized NLP by using self-attention mechanisms to model long-range dependencies across entire input sequences.')]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_1 = \"Cat is not a dog\"\n",
        "cos_similarity_calculation(text_1, sentences_list, embeddings, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ysHatbf2IdC"
      },
      "source": [
        "Create a function that determines the cosine similarity between a vector and a batch of vectors using the cosine distance formula and the numpy library. Add code to demonstrate how to use this function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4R0iPr72096H"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence 1: Distance = 0.44009023904800415\n",
            "Text: Machine learning algorithms can identify complex patterns in large datasets that would be impossible for humans to detect manually.\n",
            "\n",
            "Sentence 2: Distance = 0.735309362411499\n",
            "Text: Natural language processing enables computers to understand, interpret, and generate human language in a meaningful and useful way.\n",
            "\n",
            "Sentence 3: Distance = 0.7077240943908691\n",
            "Text: Deep neural networks are inspired by the structure of the human brain and consist of many interconnected layers of nodes.\n",
            "\n",
            "Sentence 4: Distance = 0.694071352481842\n",
            "Text: Transfer learning allows a model pre-trained on one task to be fine-tuned efficiently for a completely different target task.\n",
            "\n",
            "Sentence 5: Distance = 0.8114359378814697\n",
            "Text: Vector embeddings represent words or sentences as dense numerical arrays that capture semantic meaning and contextual relationships.\n",
            "\n",
            "Sentence 6: Distance = 0.5748833417892456\n",
            "Text: Reinforcement learning trains agents to make sequential decisions by rewarding desired behaviors and penalizing undesired ones over time.\n",
            "\n",
            "Sentence 7: Distance = 0.6206741333007812\n",
            "Text: Data preprocessing is a critical step in any machine learning pipeline because garbage input always produces garbage output results.\n",
            "\n",
            "Sentence 8: Distance = 0.7701846957206726\n",
            "Text: Transformer architectures revolutionized NLP by using self-attention mechanisms to model long-range dependencies across entire input sequences.\n",
            "\n",
            "Sentence 9: Distance = 0.8196929693222046\n",
            "Text: Semantic search retrieves documents based on the meaning of a query rather than simple keyword overlap or exact string matching.\n",
            "\n",
            "Sentence 10: Distance = 0.6851687431335449\n",
            "Text: Model evaluation metrics such as precision, recall, and F1-score help us understand how well a classifier is really performing.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# place your code here\n",
        "import numpy as np\n",
        "def cos_distance_vectors(vector, batch):\n",
        "    top = np.dot(batch, vector)\n",
        "    \n",
        "    bottom = np.linalg.norm(vector) * np.linalg.norm(batch, axis=1)\n",
        "    \n",
        "    return 1 - top / bottom\n",
        "\n",
        "text_2 = \"Machine learning algorithms\"\n",
        "embedding_text_2 = model.encode(text_2)\n",
        "distances = cos_distance_vectors(embedding_text_2, embeddings)\n",
        "\n",
        "for i, distance in enumerate(distances):\n",
        "    print(f\"Sentence {i+1}: Distance = {distance}\")\n",
        "    print(f\"Text: {sentences_list[i]}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
